{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia data transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install a magic to reload the src directory\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;33m../data/\u001b[0m\r\n",
      "├── \u001b[38;5;33mexternal\u001b[0m\r\n",
      "├── \u001b[38;5;33minterim\u001b[0m\r\n",
      "├── \u001b[38;5;33mprocessed\u001b[0m\r\n",
      "└── \u001b[38;5;33mraw\u001b[0m\r\n",
      "    └── \u001b[38;5;40menwiki-20080103.main.bz2\u001b[0m\r\n",
      "\r\n",
      "4 directories, 1 file\r\n"
     ]
    }
   ],
   "source": [
    "!tree ../data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data into Spark. Reading through all the data by counting takes ~2 hours with 8 cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "conf = sc._jsc.hadoopConfiguration()\n",
    "conf.set(\"textinputformat.record.delimiter\", \"\\n\\n\")\n",
    "\n",
    "input_file = 'enwiki-20080103.main.bz2'\n",
    "rdd = sc.textFile(os.path.join(\"../data/raw\", input_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark has dataframe support like dyplr or pandas. We add typing and partition to reduce the overhead of processing this a second time.\n",
    "\n",
    "We can programatically access the schemas, used when coercing string values into the correct type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IntegerType"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.data import import_wikipedia as impwiki\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "# how to access the datatype of a schema\n",
    "schema = impwiki.wikipedia_schema\n",
    "schema['article_id'].dataType\n",
    "\n",
    "# how we can add typing easily to new string data\n",
    "# TODO: test case\n",
    "\n",
    "cast_map = {\n",
    "    T.IntegerType: int,\n",
    "    T.BooleanType: bool,\n",
    "    T.StringType: str,\n",
    "}\n",
    "\n",
    "for name in impwiki.wikipedia_schema.fieldNames():\n",
    "    cast_map[type(imw.wikipedia_schema[name].dataType)](0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the schema and create a temporary table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " article_id    | 6                    \n",
      " rev_id        | 233188               \n",
      " article_title | AmericanSamoa        \n",
      " timestamp     | 2001-01-19T01:12:51Z \n",
      " username      | ip:office.bomis.com  \n",
      " user_id       | ip:office.bomis.com  \n",
      " category      | null                 \n",
      " image         | null                 \n",
      " main          | null                 \n",
      " talk          | null                 \n",
      " user          | null                 \n",
      " user_talk     | null                 \n",
      " other         | null                 \n",
      " external      | null                 \n",
      " template      | null                 \n",
      " comment       | *                    \n",
      " minor         | true                 \n",
      " textdata      | 1516                 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enwiki_df = spark.createDataFrame(\n",
    "    rdd.map(impwiki.process_edit), \n",
    "    schema=impwiki.wikipedia_schema\n",
    ")\n",
    "\n",
    "enwiki_df.createOrReplaceTempView('enwiki')\n",
    "enwiki_df.show(truncate=False, vertical=True, n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let there be light. Here's the beginning of Wikipedia using Spark as a query engine. But if we take a second look, the timestamps are out of order and we're starting off at revision 233188."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---------+--------------------+--------------------+-----+--------+\n",
      "|           timestamp|article_id|   rev_id|             user_id|            username|minor|textdata|\n",
      "+--------------------+----------+---------+--------------------+--------------------+-----+--------+\n",
      "|2001-01-19T01:12:51Z|         6|   233188| ip:office.bomis.com| ip:office.bomis.com| true|    1516|\n",
      "|2007-05-24T14:41:33Z|         6|133180191|             4477979|            Ngaiklin| true|       5|\n",
      "|2001-01-20T15:01:12Z|         8|   233189|ip:pD950754B.dip....|ip:pD950754B.dip....| true|       9|\n",
      "|2007-05-24T14:41:48Z|         8|133180238|             4477979|            Ngaiklin| true|       6|\n",
      "|2001-01-21T02:12:21Z|        10|   233192|                  99|           RoseParks| true|       8|\n",
      "|2007-05-24T14:41:58Z|        10|133180268|             4477979|            Ngaiklin| true|       6|\n",
      "|2002-02-25T15:00:22Z|        12|    18201|ip:Conversion_script|ip:Conversion_script| true|    1214|\n",
      "|2002-02-25T15:43:11Z|        12|    19746|   ip:140.232.153.45|   ip:140.232.153.45| true|    1460|\n",
      "|2002-02-27T17:34:09Z|        12|    19749|    ip:24.188.31.147|    ip:24.188.31.147| true|    1474|\n",
      "|2002-02-27T17:36:41Z|        12|    20514|    ip:24.188.31.147|    ip:24.188.31.147| true|    1483|\n",
      "|2002-03-01T00:13:17Z|        12|    42733|   ip:213.253.39.175|   ip:213.253.39.175| true|    1483|\n",
      "|2002-04-02T09:51:25Z|        12|    42738|     ip:206.82.16.35|     ip:206.82.16.35| true|    1489|\n",
      "|2002-04-02T09:53:06Z|        12|    42740|     ip:206.82.16.35|     ip:206.82.16.35| true|    1488|\n",
      "|2002-04-02T09:54:12Z|        12|    42743|     ip:206.82.16.35|     ip:206.82.16.35| true|    1483|\n",
      "|2002-04-02T09:55:36Z|        12|    43618|                  43|  Lee_Daniel_Crocker| true|    1468|\n",
      "|2002-07-18T14:33:46Z|        12|    59361|                 170|                   0| true|    1169|\n",
      "|2002-04-26T07:02:43Z|        12|    61039|    ip:80.65.225.191|    ip:80.65.225.191| true|    1504|\n",
      "|2002-05-01T14:18:43Z|        12|    61179|                 372|       Eclecticology| true|    1506|\n",
      "|2002-05-01T20:54:35Z|        12|    61193|                 372|       Eclecticology| true|    1507|\n",
      "|2002-04-03T07:36:30Z|        12|    67475|                  17|      Peter_Winnberg| true|    1468|\n",
      "+--------------------+----------+---------+--------------------+--------------------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    timestamp,\n",
    "    article_id, \n",
    "    rev_id,\n",
    "    user_id,\n",
    "    username,\n",
    "    minor,\n",
    "    textdata\n",
    "FROM \n",
    "    enwiki\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double check that the timestamps are actually out of order after casting into a timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          timestamp|\n",
      "+-------------------+\n",
      "|2001-01-18 17:12:51|\n",
      "|2007-05-24 07:41:33|\n",
      "|2001-01-20 07:01:12|\n",
      "|2007-05-24 07:41:48|\n",
      "|2001-01-20 18:12:21|\n",
      "|2007-05-24 07:41:58|\n",
      "+-------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT cast(timestamp as TIMESTAMP) FROM enwiki\").show(n=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|     min(timestamp)|     max(timestamp)|\n",
      "+-------------------+-------------------+\n",
      "|2001-01-18 17:12:51|2007-05-24 07:41:58|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# take a few values\n",
    "(\n",
    "    enwiki_df\n",
    "    .select(F.col('timestamp').cast('timestamp'))\n",
    "    .limit(1000)\n",
    "    .select(F.min('timestamp'), F.max('timestamp'))\n",
    ").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
