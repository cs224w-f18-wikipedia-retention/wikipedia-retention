{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/amiyaguchi/wikipedia-retention\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007-1-enwiki-projection-user.csv\t all_user_features.csv\r\n",
      "2007-1-enwiki-projection-user-roles.csv  base_features_reg.csv\r\n",
      "2007Q1-user-network-v1.csv\t\t community_norm_features.csv\r\n",
      "2007Q1-user-network-v2.csv\t\t enwiki-meta-compact\r\n",
      "all_article_features.csv\t\t kcore-2007-1.csv\r\n"
     ]
    }
   ],
   "source": [
    "! ls data/processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.snap_import_user_projection import UnimodalUserProjection\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "input_path = \"data/processed/enwiki-meta-compact\"\n",
    "model = UnimodalUserProjection(spark).extract(input_path).transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def write_csv(df, name, overwrite=False):\n",
    "    interim_path = \"data/interim/{}\".format(name)\n",
    "    processed_file = \"data/processed/{}.csv\".format(name)\n",
    "    if os.path.exists(processed_file) and not overwrite:\n",
    "        print(\"file already exists\")\n",
    "        return\n",
    "    df.repartition(1).write.csv(interim_path, header=True, mode=\"overwrite\")\n",
    "    interim_file = glob.glob(\"{}/*.csv\".format(interim_path))[0]\n",
    "    shutil.copy(interim_file, processed_file)\n",
    "    shutil.rmtree(interim_path)\n",
    "    print(\"wrote file to {}\".format(processed_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53599976"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.table(\"bipartite\").cache()\n",
    "spark.table(\"bipartite\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19362468"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projection = spark.sql(\"\"\"\n",
    "select\n",
    "    t1.user_id as e1,\n",
    "    t2.user_id as e2,\n",
    "    t1.article_id\n",
    "from bipartite t1\n",
    "join bipartite t2\n",
    "on t1.article_id = t2.article_id\n",
    "where t1.user_id < t2.user_id\n",
    "    and t1.edit_date = t2.edit_date\n",
    "\"\"\")\n",
    "\n",
    "projection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists\n"
     ]
    }
   ],
   "source": [
    "write_csv(projection, \"user-network-intermediate-edgelist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44869198"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_list = spark.sql(\"\"\"\n",
    "with block_list as (\n",
    "    select\n",
    "        article_id,\n",
    "        edit_date,\n",
    "        collect_set(user_id) as user_set\n",
    "    from bipartite\n",
    "    group by 1, 2\n",
    ")\n",
    "select \n",
    "    article_id,\n",
    "    edit_date,\n",
    "    size(user_set) as n_users,\n",
    "    user_set\n",
    "from block_list\n",
    "\"\"\")\n",
    "\n",
    "block_list.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    block_list\n",
    "    .repartition(1)\n",
    "    .write.parquet(\"data/processed/user-network-intermediate-block-userset\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
