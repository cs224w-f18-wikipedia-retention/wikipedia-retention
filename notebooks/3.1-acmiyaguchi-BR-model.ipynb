{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/amiyaguchi/cs224w/wikipedia-retention\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",role_-1,role_-1,role_0,role_1,role_2,role_3,role_4,role_5,role_6,role_7,role_8,role_9,role_10,role_11,role_12,role_13,role_14,role_15,role_16,role_17,role_18,role_19,role_20,role_21,role_22,role_23,role_24,role_25,role_26,role_27,role_28,role_29,role_30\r\n",
      "0,0.0,0.0,3.385962476763866e-06,7.593783790784927e-05,0.0,5.112422163371725e-06,6.250467324660294e-05,2.5913449080068567e-06,3.781486786067123e-06,1.2189651473482883e-06,0.0,0.0,3.8132232413406166e-06,2.830387035209037e-06,1.0047484411328175e-06,0.0,1.1455070825746656e-06,7.518268179048739e-06,0.0,1.9660844809643654e-06,3.240578019407499e-06,0.0,0.0,2.044498092073995e-06,2.5600785666519907e-06,6.661282512800984e-06,7.815884136577666e-06,1.3817765777815095e-06,3.245648475330179e-06,3.5783210994510185e-06,5.420547778876492e-06,3.603760080559225e-06,0.0\r\n"
     ]
    }
   ],
   "source": [
    "! head -n2 data/processed/role-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amiyaguchi/cs224w/wikipedia-retention/venv3/lib/python3.6/site-packages/ipykernel_launcher.py:29: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "# Model for neural net regression on 2007-Q1\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "# load features\n",
    "base = \"data/processed/\"\n",
    "uf_name = base + 'base_features_reg.csv'\n",
    "af_name = base + 'all_article_features.csv'\n",
    "rolef_name = base + 'role-features'\n",
    "cf_name = base + 'community_norm_features.csv'\n",
    "\n",
    "# process roles\n",
    "role_df = pd.read_csv(rolef_name, header=None, skiprows=1)\n",
    "\n",
    "user_df = pd.read_csv(uf_name, header=None)\n",
    "# mark columns that are 2007-q1\n",
    "time_str = '2007-01-01T00:00:00.000-08:00'\n",
    "time_idx = np.where(user_df[1] == '2007-01-01T00:00:00.000-08:00')[0]\n",
    "y = np.ndarray.astype(user_df.values[:,-1],int)\n",
    "user_df = user_df.drop([1,user_df.columns[-1]],axis=1) # drop time and y column\n",
    "article_df = pd.read_csv(af_name, header=None)\n",
    "community_norm_df = pd.read_csv(cf_name, header=None)\n",
    "\n",
    "# process joined data\n",
    "ua_df = user_df.merge(article_df, on=0)\n",
    "uac_df = ua_df.merge(community_norm_df, on=0)\n",
    "X_df = uac_df.merge(role_df, how='left', on=0) \n",
    "X = X_df.as_matrix()\n",
    "# strip by time_idx\n",
    "X = X[time_idx,:]\n",
    "y = y[time_idx]\n",
    "# set missing roles to 1 (in what used to be role_id)\n",
    "X[np.isnan(X[:,40]),40] = 1\n",
    "X = np.ndarray.astype(X[:,1:],float) # remove user_id\n",
    "X[np.isnan(X)] = 0 # clear NaNs\n",
    "# add new column for log(sum(log(textdata)))\n",
    "lslt = np.array([np.log(X[:,5])+1]).T\n",
    "X = np.append(X, lslt, 1)\n",
    "# min-max scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scalar = MinMaxScaler(feature_range=(0,1))\n",
    "scalar.fit(X)\n",
    "dmin = scalar.data_min_\n",
    "dmax = scalar.data_max_\n",
    "Xnorm = scalar.transform(X)\n",
    "Xnorm = Xnorm - Xnorm.mean(axis=0)\n",
    "\n",
    "# NOTE: Above is mostly copy paste from logreg, after here is NN.\n",
    "# We should probably try to standardized our feature sets to make joining easier\n",
    "\n",
    "# setup train-test split. might want train-dev-test for final model testing\n",
    "# could also rebalance (since so many 0 examples)\n",
    "yl = np.log(y+1) # run on log y for smoother fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.metrics import log_loss, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# train model\n",
    "def fit_model(model,X,y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=115)\n",
    "    model.fit(X_train, y_train)\n",
    "    score = model.score(X_test,y_test) # can have sample weight here\n",
    "    return model, score\n",
    "\n",
    "def plot_preds(preds, y, xlab='prediction', ylab='actual contribution'):\n",
    "    plt.plot(np.exp(preds-1),np.exp(y-1),'.')\n",
    "    mx = np.exp(min(np.max(preds),np.max(y)))\n",
    "    plt.plot([1,mx],[1,mx],color='red')\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel(xlab)\n",
    "    plt.ylabel(ylab)\n",
    "    plt.show()\n",
    "\n",
    "def plot_yl(yl):\n",
    "    plt.hist(yl,log=True)\n",
    "    plt.xlabel('log contribution')\n",
    "    plt.ylabel('bin count')\n",
    "    plt.show()\n",
    "\n",
    "# NOTE: Below is the full 2-model (class * reg)\n",
    "alphas = [0.1, 1, 10, 100]\n",
    "nodes = [(3), (5), (7), (9), (8, 3), (10, 3), (12, 4), (14, 5)]\n",
    "#alphas = [0.1]\n",
    "#nodes = [(8,3)]\n",
    "params = {\"alpha\" : alphas, \"hidden_layer_sizes\" : nodes}\n",
    "theta_idx = yl > 0\n",
    "yl_theta = yl[theta_idx]\n",
    "X_theta = Xnorm[theta_idx] # note: don't rescale since we need to combine models\n",
    "# run regression model\n",
    "MLPR = MLPRegressor(activation = 'relu', solver = 'adam', random_state = 112358)\n",
    "GSR = GridSearchCV(MLPR, params, return_train_score = True)\n",
    "reg_model, reg_score = fit_model(GSR, X_theta, yl_theta)\n",
    "rm = reg_model.best_estimator_\n",
    "reg_preds = rm.predict(Xnorm) # predict on all\n",
    "# run classification model\n",
    "MLPC = MLPClassifier(activation='relu', solver='adam', random_state = 112358)\n",
    "GSC = GridSearchCV(MLPC, params, return_train_score = True)\n",
    "theta = np.ndarray.astype(theta_idx,int)\n",
    "class_model, class_score = fit_model(GSC, Xnorm, theta)\n",
    "cm = class_model.best_estimator_\n",
    "class_preds = cm.predict_proba(Xnorm)[:,1]\n",
    "# now combine\n",
    "combined_preds = class_preds * reg_preds\n",
    "combined_score = sklearn.metrics.r2_score(yl,combined_preds)\n",
    "\n",
    "# plot class hist\n",
    "def plot_class_hist(theta_idx, class_preds):\n",
    "    plt.hist([1-class_preds[theta_idx],class_preds[~theta_idx]],label=['theta=1','theta=0'],log=True,bins=20)\n",
    "    plt.xlabel(\"Classification error\")\n",
    "    plt.ylabel(\"Bin Count\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29616916808554106"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
