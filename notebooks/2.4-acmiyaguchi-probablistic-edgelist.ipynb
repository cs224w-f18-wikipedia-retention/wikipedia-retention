{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/amiyaguchi/cs224w/wikipedia-retention\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k,p_one,p_any\r\n",
      "2 0.99000000 0.99498744\r\n",
      "3 0.90000000 0.94216829\r\n",
      "4 0.78455653 0.86410869\r\n",
      "5 0.68377223 0.78831332\r\n",
      "6 0.60189283 0.72155904\r\n",
      "7 0.53584112 0.66416306\r\n",
      "8 0.48205253 0.61492493\r\n",
      "9 0.43765867 0.57247457\r\n",
      "10 0.40051575 0.53560803\r\n"
     ]
    }
   ],
   "source": [
    "!head data/processed/markov_bounds.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.snap_import_user_projection import UnimodalUserProjection\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "input_path = \"data/processed/enwiki-meta-compact\"\n",
    "model = UnimodalUserProjection(spark).extract(input_path).transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.2 ms, sys: 7.06 ms, total: 19.3 ms\n",
      "Wall time: 1min 40s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13713183"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_list = spark.sql(\"\"\"\n",
    "with block_list as (\n",
    "    select\n",
    "        article_id,\n",
    "        concat(year(edit_date), '-', quarter(edit_date)) as edit_date,\n",
    "        collect_set(user_id) as user_set\n",
    "    from bipartite\n",
    "    group by 1,2\n",
    ")\n",
    "select \n",
    "    article_id,\n",
    "    edit_date,\n",
    "    size(user_set) as n_users,\n",
    "    user_set\n",
    "from block_list\n",
    "\"\"\")\n",
    "\n",
    "block_list.cache()\n",
    "%time block_list.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- article_id: integer (nullable = true)\n",
      " |-- edit_quarter: string (nullable = true)\n",
      " |-- n_users: integer (nullable = false)\n",
      " |-- user_set: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "block_list.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|max(size(user_set))|\n",
      "+-------------------+\n",
      "|               1732|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "block_list.selectExpr(\"max(size(user_set))\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate markov bounds for cliques of size 1-n based on variables\n",
    "from scipy.optimize import fsolve\n",
    "import numpy as np\n",
    "\n",
    "n = 1732\n",
    "epsilon = 0.01\n",
    "\n",
    "# loop over all n using previous value as seed\n",
    "any_bound = {}\n",
    "all_bound = {}\n",
    "p_one = 1\n",
    "p_all = 1\n",
    "for k in range(2,n+1):\n",
    "    func_one = lambda p: ((1-p) ** (k-1)) / epsilon - 1\n",
    "    func_any = lambda p: (1 - ((1- ((1-p) ** (k-1))) ** k)) / epsilon - 1\n",
    "    p_one = fsolve(func_one,p_one)[0]\n",
    "    p_all = fsolve(func_any,p_all)[0]\n",
    "    any_bound[k] = p_one\n",
    "    all_bound[k] = p_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|sum(n_edges)|\n",
      "+------------+\n",
      "|2.82368293E8|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@F.udf(T.ArrayType(T.ArrayType(T.IntegerType())))\n",
    "def all_edges(user_set):\n",
    "    return list(combinations(sorted(user_set), 2))\n",
    "\n",
    "block_list.selectExpr(\"n_users*(n_users-1)/2 as n_edges\").selectExpr(\"sum(n_edges)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types as T\n",
    "from itertools import combinations\n",
    "from random import random\n",
    "\n",
    "bounds = any_bound\n",
    "\n",
    "@F.udf(T.ArrayType(T.ArrayType(T.IntegerType())))\n",
    "def sample_edges(user_set):\n",
    "    k = len(user_set)\n",
    "    if k < 2:\n",
    "        return []\n",
    "    p = bounds[k]\n",
    "    edges = [c for c in combinations(sorted(user_set), 2) if random() < p]\n",
    "    return edges\n",
    "\n",
    "edgelist = (\n",
    "    block_list\n",
    "    .select(F.explode(sample_edges(\"user_set\")).alias(\"edges\"))\n",
    "    .select(F.col(\"edges\").getItem(0).alias(\"e1\"), F.col(\"edges\").getItem(1).alias(\"e2\"))\n",
    "    .groupby(\"e1\", \"e2\")\n",
    "    .agg(F.expr(\"count(*) as weight\"))\n",
    ")\n",
    "\n",
    "edgelist.repartition(1).write.parquet(\"data/processed/user-network-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+\n",
      "|     e1|     e2|weight|\n",
      "+-------+-------+------+\n",
      "| 571653| 587577|     1|\n",
      "| 422965|1467795|     2|\n",
      "| 612852|1725149|     7|\n",
      "|   8029|  29856|    19|\n",
      "|1218374|3447299|     1|\n",
      "|  35314|  57658|     8|\n",
      "| 271058|2128469|     3|\n",
      "| 743015| 899701|    11|\n",
      "| 109883| 234523|     1|\n",
      "|1215485|2407864|     1|\n",
      "|  14010|2336102|     1|\n",
      "| 801279|1538132|     1|\n",
      "| 959742|1251026|     2|\n",
      "| 293907|1122589|     1|\n",
      "| 304736| 603177|     2|\n",
      "|2090843|2654847|     4|\n",
      "| 521374| 642738|     1|\n",
      "|  28107|  87543|     4|\n",
      "|  81704|1587622|     1|\n",
      "| 266416|1575512|    49|\n",
      "+-------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edgelist = spark.read.parquet(\"data/processed/user-network-v3\")\n",
    "edgelist.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
